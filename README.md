# ğŸ° Age of Empires Human-Guided RL

Este repositÃ³rio explora **Aprendizado por ReforÃ§o (Reinforcement Learning)** combinado com **Aprendizado por DemonstraÃ§Ã£o (Imitation Learning / Inverse Reinforcement Learning)** aplicado ao jogo **Age of Empires II: Definitive Edition**.

A ideia central Ã© **observar a performance humana** em *replays* para:
- Extrair *trajectories* (estado â†’ aÃ§Ã£o macro).
- Treinar uma polÃ­tica inicial por **ImitaÃ§Ã£o** (Behavioral Cloning / GAIL).
- Refinar essa polÃ­tica com **Reinforcement Learning** em ambiente controlado ou diretamente no AoE2.

---

## ğŸ¯ Objetivos

- Aprender *build orders* e transiÃ§Ãµes a partir de replays humanos.
- Criar agentes capazes de executar **estratÃ©gias plausÃ­veis**, em vez de micro perfeito.
- Evoluir de **demonstraÃ§Ãµes** para **self-play com recompensas densas**.

---

## ğŸ› ï¸ Stack TÃ©cnica

- **Parsing de Replays**: [aoc-mgz](https://github.com/happyleavesaoc/aoc-mgz) (suporta `.aoe2record`).
- **RL / IL**: [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3), [imitation](https://github.com/HumanCompatibleAI/imitation).
- **Ambientes**:
  - Prototipagem: [ÂµRTS](https://github.com/vwxyzjn/gym-microrts) (RTS minimalista com interface Gym).
  - ExecuÃ§Ã£o real: automaÃ§Ã£o via OCR/hotkeys ou [openage](https://github.com/SFTtech/openage).
- **VisualizaÃ§Ã£o**: [CaptureAge](https://www.captureage.com/) para depuraÃ§Ã£o.

---

## ğŸ“‚ Estrutura

```
â”œâ”€â”€ data/                # Replays e trajectories processadas
â”œâ”€â”€ notebooks/           # AnÃ¡lises e protÃ³tipos
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ parsing/         # Conversores .aoe2record -> dataset
â”‚   â”œâ”€â”€ macros/          # DefiniÃ§Ã£o de aÃ§Ãµes em nÃ­vel macro
â”‚   â”œâ”€â”€ policies/        # Modelos (BC, GAIL, PPO, SAC)
â”‚   â”œâ”€â”€ envs/            # Wrappers (ÂµRTS, AoE2 OCR, openage)
â”‚   â””â”€â”€ rewards/         # FunÃ§Ãµes de recompensa densas
â””â”€â”€ README.md
```

---

## ğŸš€ Pipeline

1. **Coleta** de replays humanos (`.aoe2record`).
2. **Parsing** â†’ estados agregados (recursos, upgrades, army size, timings).
3. **Mapeamento de AÃ§Ãµes Macro** â†’ `Train(Villager)`, `AgeUp(Feudal)`, `Build(ArcheryRange)`, etc.
4. **ImitaÃ§Ã£o**:
   - *Behavioral Cloning* (supervisionado).
   - *GAIL* para maior robustez.
5. **Reinforcement Learning**:
   - *Offline RL*: CQL/IQL sobre os replays.
   - *Online RL*: PPO/SAC com recompensas densas (eco, army, timings).
6. **ValidaÃ§Ã£o**:
   - Timings (Feudal, Castle, Imperial).
   - Uptime de TCs e Idle Villagers.
   - ExecuÃ§Ã£o de *build orders* plausÃ­veis.

---

## ğŸ“Š Exemplo de Feature (estado)

```json
{
  "time": 410,
  "food": 350,
  "wood": 180,
  "gold": 0,
  "villagers": 19,
  "idle_tc": 0,
  "age": "Dark",
  "military": {"maa": 2, "archers": 0},
  "upgrades": ["loom"],
  "enemy_pressure": 0.2
}
```

---

## ğŸ“¦ InstalaÃ§Ã£o

```bash
git clone https://github.com/SEU_USUARIO/aoe2-human-rl.git
cd aoe2-human-rl
pip install -r requirements.txt
```

Requisitos principais:
- Python 3.9+
- `stable-baselines3`
- `imitation`
- `torch`
- `aoc-mgz`
- `gym-microrts` (opcional)

---

## ğŸ“… Roadmap

- [ ] Parsing inicial de replays â†’ CSV/JSON de trajectories.  
- [ ] DefiniÃ§Ã£o de espaÃ§o de aÃ§Ã£o macro.  
- [ ] Treino com Behavioral Cloning.  
- [ ] GAIL para robustez.  
- [ ] Recompensas densas macro-aware.  
- [ ] IntegraÃ§Ã£o com openage.  
- [ ] Self-play hÃ­brido IL + RL.  

---

## ğŸ“š ReferÃªncias

- Silver et al., *Mastering the game of Go with deep neural networks and tree search*, Nature (2016).
- Ho & Ermon, *Generative Adversarial Imitation Learning* (2016).
- Gym-MicroRTS: https://github.com/vwxyzjn/gym-microrts
- Projeto openage: https://github.com/SFTtech/openage
- Biblioteca aoc-mgz: https://github.com/happyleavesaoc/aoc-mgz

---

## ğŸ¤ ContribuiÃ§Ãµes

Pull requests e sugestÃµes sÃ£o bem-vindos! Se quiser discutir ideias, abra uma issue.
